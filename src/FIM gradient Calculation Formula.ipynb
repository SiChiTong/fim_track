{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad,jit, jacfwd\n",
    "from matplotlib import pyplot as plt\n",
    "import jax.numpy as jnp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will implement two versions of calculating gradient of L. One using autodifferentiation with jax, the other following the analytic formula we derived. In this way we can compare them and have a sanity check on the correctness our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad,jit, jacfwd\n",
    "from matplotlib import pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def single_meas_func(C1,C0,k,b,dist):\n",
    "    \"\"\"\n",
    "        The small h function, for each individual measurement.\n",
    "    \"\"\"\n",
    "    return k*jnp.power(dist-C1,b)+C0\n",
    "\n",
    "\n",
    "def joint_meas_func(C1s,C0s,ks,bs,q,ps):\n",
    "    \"\"\"\n",
    "        The big H function, the array of all individual measurements.\n",
    "    \"\"\"\n",
    "\n",
    "    # Casting for the compatibility of jax.numpy\n",
    "\n",
    "    C1s=jnp.array(C1s)\n",
    "    C0s=jnp.array(C0s)\n",
    "    ks=jnp.array(ks)\n",
    "    bs=jnp.array(bs)\n",
    "    ps=jnp.array(ps)\n",
    "\n",
    "    # Keep in mind that x is a vector of [q,q'], thus only the first half of components are observable.    \n",
    "    dists=jnp.linalg.norm(q-ps,axis=1)\n",
    "\n",
    "    return single_meas_func(C1s,C0s,ks,bs,dists) \n",
    "\n",
    "\n",
    "def FIM(q,ps,sigma,C1s,C0s,ks,bs):\n",
    "    \"\"\"\n",
    "       The computation of Fish Information Matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    H=partial(joint_meas_func, C1s,C0s,ks,bs)\n",
    "    \n",
    "    # Taking partial derivative of H w.r.t. the zeroth argument, which is q.\n",
    "    dHdq=jit(jacfwd(H,argnums=0))\n",
    "    return 1/(jnp.power(sigma,2)) *  dHdq(q,ps).T.dot(dHdq(q,ps))\n",
    "\n",
    "def L(q,ps,sigma,C1s,C0s,ks,bs):\n",
    "    \"\"\"\n",
    "        The reward function big L. It is just det(FIM)\n",
    "    \"\"\"\n",
    "    \n",
    "    return jnp.linalg.det(FIM(q,ps,sigma,C1s,C0s,ks,bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def analytic_L(q,ps,sigma,C1s,C0s,ks,bs):\n",
    "    n_p=len(ps)\n",
    "    r=jnp.linalg.norm(ps-q,axis=1).reshape(-1,1)\n",
    "    r_hat=(ps-q)/r\n",
    "\n",
    "    L=0\n",
    "    for i in range(n_p):\n",
    "        for j in range(n_p):\n",
    "                \n",
    "            rkrj=jnp.min([r_hat[i,:].dot(r_hat[j,:]),1])\n",
    "            \n",
    "            L+=(bs[i]*bs[j]*ks[i]*ks[j])**2 * (r[i]-C1s[i])**(2*bs[i]-2) * (r[j]-C1s[j])**(2*bs[j]-2) * (1-rkrj**2)\n",
    "            \n",
    "    L/=2*sigma**2\n",
    "    \n",
    "    return L[0]\n",
    "def analytic_dLdp(q,ps,sigma,C1s,C0s,ks,bs):\n",
    "\n",
    "    n_p=len(ps)\n",
    "    r=np.linalg.norm(ps-q,axis=1).reshape(-1,1)\n",
    "    r_hat=(ps-q)/r\n",
    "    t_hat=np.zeros(rhat.shape)\n",
    "    t_hat[:,0]=-r_hat[:,1]\n",
    "    t_hat[:,1]=r_hat[:,0]\n",
    "\n",
    "    dLdeta=np.zeros(n_p).reshape(-1,1)\n",
    "    dLdr=np.zeros(n_p).reshape(-1,1)\n",
    "\n",
    "\n",
    "    for i in range(n_p):\n",
    "        Keta=2*(ks[i]*bs[i])**2/(sigma**2) * (r[i]-C1s[i])**(2*bs[i]-2)\n",
    "        Kr=2*(ks[i]*bs[i])**2/(sigma**2) * (bs[i]-1) * (r[i]-C1s[i])**(2*bs[i]-3)\n",
    "        sum_eta=sum_kr=0\n",
    "        for j in range(n_p):\n",
    "                \n",
    "            rkrj=np.min([r_hat[i,:].dot(r_hat[j,:]),1])\n",
    "            \n",
    "            direction=np.sign(np.linalg.det(r_hat[[j,i],:]))\n",
    "\n",
    "            sum_eta += (ks[j]*bs[j])**2 * (r[j]-C1s[j])**(2*bs[j]-2) * rkrj * np.sqrt(1-rkrj**2) * direction\n",
    "            sum_kr += (ks[j]*bs[j])**2 * (r[j]-C1s[j])**(2*bs[j]-2) * (1-rkrj**2)\n",
    "        \n",
    "        dLdeta[i]=Keta*sum_eta\n",
    "        dLdr[i]=Kr*sum_kr\n",
    "        \n",
    "    dLdp = dLdr * r_hat  + (dLdeta/r) * t_hat\n",
    "    \n",
    "    \n",
    "    return dLdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A sanity check is when b=1, and the sensors are distributed around the target equi-angularly, then the Jacobians should be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [-0.  0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from dLdp import dLdp,analytic_dLdp\n",
    "import jax.numpy as jnp\n",
    "sigma=1.\n",
    "C0s=jnp.array([0.,0.,0.])\n",
    "C1s=jnp.array([0.1,0.1,0.1])\n",
    "ks=jnp.array([1.5,0.5,1.])\n",
    "bs=jnp.array([1.,1.,1.])\n",
    "\n",
    "q=jnp.array([0.,0.])\n",
    "ps=jnp.array([[1.,0],[0,1.],[-1,0.0]])\n",
    "\n",
    "# print(L(q,ps,sigma,C1s,C0s,ks,bs))\n",
    "# print(analytic_L(q,ps,sigma,C1s,C0s,ks,bs))\n",
    "\n",
    "# print(jit(grad(analytic_L,argnums=1))(q,ps,sigma,C1s,C0s,ks,bs))\n",
    "# print(jit(grad(L,argnums=1))(q,ps,sigma,C1s,C0s,ks,bs))\n",
    "\n",
    "print(analytic_dLdp(q,ps,sigma,C1s,C0s,ks,bs))\n",
    "print(dLdp(q,ps,sigma,C1s,C0s,ks,bs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When b=-2, and the sensors are equi-angularly located, the gradient should all point towards the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   -0.      -1062.2124 ]\n",
      " [ -212.44246     0.     ]\n",
      " [  849.76984    -0.     ]]\n",
      "[[    0.      -1062.2124 ]\n",
      " [ -212.44247     0.     ]\n",
      " [  849.7699      0.     ]]\n"
     ]
    }
   ],
   "source": [
    "sigma=1.\n",
    "C0s=jnp.array([0.,0.,0.])\n",
    "C1s=jnp.array([0.1,0.1,0.1])\n",
    "ks=jnp.array([1.5,0.5,1.])\n",
    "bs=jnp.array([-2.,-2.,-2.])\n",
    "\n",
    "q=jnp.array([0.,0.])\n",
    "ps=jnp.array([[0,1.],[1.,0],[-1,0]])\n",
    "\n",
    "# print(L(q,ps,sigma,C1s,C0s,ks,bs))\n",
    "# print(analytic_L(q,ps,sigma,C1s,C0s,ks,bs))\n",
    "\n",
    "# print(jit(grad(analytic_L,argnums=1))(q,ps,sigma,C1s,C0s,ks,bs))\n",
    "# print(jit(grad(L,argnums=1))(q,ps,sigma,C1s,C0s,ks,bs))\n",
    "\n",
    "print(analytic_dLdp(q,ps,sigma,C1s,C0s,ks,bs))\n",
    "print(dLdp(q,ps,sigma,C1s,C0s,ks,bs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When b=-2 and the sensors are not well separated, the gradient should make them separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 18.31319   -10.174047 ]\n",
      " [ 10.593998   -6.5559745]\n",
      " [-28.121792  -14.409895 ]]\n",
      "[[ 18.313099  -10.173941 ]\n",
      " [ 10.593956   -6.5559235]\n",
      " [-28.121672  -14.409692 ]]\n"
     ]
    }
   ],
   "source": [
    "sigma=1.\n",
    "C0s=jnp.array([0.,0.,0.])\n",
    "C1s=jnp.array([0.1,0.1,0.1])\n",
    "ks=jnp.array([1.5,0.5,1.])\n",
    "bs=jnp.array([-2.,-2.,-2.])\n",
    "\n",
    "q=jnp.array([0.,0.])\n",
    "ps=jnp.array([[0,1.],[0.1,1.],[-0.1,1]])\n",
    "\n",
    "# print(L(q,ps,sigma,C1s,C0s,ks,bs))\n",
    "# print(analytic_L(q,ps,sigma,C1s,C0s,ks,bs))\n",
    "\n",
    "# print(jit(grad(analytic_L,argnums=1))(q,ps,sigma,C1s,C0s,ks,bs))\n",
    "# print(jit(grad(L,argnums=1))(q,ps,sigma,C1s,C0s,ks,bs))\n",
    "\n",
    "\n",
    "print(analytic_dLdp(q,ps,sigma,C1s,C0s,ks,bs))\n",
    "print(dLdp(q,ps,sigma,C1s,C0s,ks,bs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So far the FIM gradients computed by jax and analytic formula are consistent. We can use either of them to do gradient calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
